{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from review_dataset import ReviewDataset\n",
    "from review_classifier import ReviewClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cuda:0'):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and path inpformation\n",
    "    frequency_cutoff = 25,\n",
    "    model_state_file = 'yelp_clf.pth',\n",
    "    review_csv='../Data/reviews_with_splits_lite.csv',\n",
    "    save_dir = './model/',\n",
    "    vectorizer_file = 'vectorizer,json',\n",
    "    # No model hyoerparameters\n",
    "    #Training hyperparameters \n",
    "    batch_size = 18,\n",
    "    early_stopping_criteria = 5,\n",
    "    learning_rate = 0.01,\n",
    "    seed = 1337,\n",
    "    num_epochs = 50, \n",
    "    cuda = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'epoch_index': 0,\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'test_loss': -1,\n",
    "    'test_acc': -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct/len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and vectorizer\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.review_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab)).to(args.device)\n",
    "print(classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch\n",
    "    \n",
    "    # batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # setout the zero grad \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute logits \n",
    "        y_logits = classifier(x=batch_dict['x_data'].float())\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = loss_func(y_logits, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index +1)\n",
    "        \n",
    "        y_preds = torch.round(torch.sigmoid(y_logits))\n",
    "        \n",
    "        # use loss to compute the gradients \n",
    "        loss.backward()\n",
    "        \n",
    "        # optimzer to take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch accuracy\n",
    "        batch_acc = compute_accuracy(y_true=batch_dict['y_target'], y_pred=y_preds)\n",
    "        running_acc += (batch_acc -running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    # vaildation iteration\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the logits \n",
    "        y_logits = classifier(x=batch_dict['x_data'].float())\n",
    "        \n",
    "        loss = loss_func(y_logits, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        \n",
    "        y_preds = torch.round(torch.sigmoid(y_logits))\n",
    "        \n",
    "        running_loss += (loss_batch -running_loss) / (batch_index +1 )\n",
    "        \n",
    "        # compute acc\n",
    "        batch_acc = compute_accuracy(y_true=batch_dict['y_target'], y_pred=y_preds)\n",
    "        running_acc += (batch_acc - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"Epoch: {epoch} \\n Train Loss:{train_state['train_loss'][-1]:.3f} | Train acc: {train_state['train_acc'][-1]:.3f} | Val loss: {train_state['val_loss'][-1]:.3f} | Val acc: {train_state['val_acc'][-1]:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_logits = classifier(x=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_logits, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    \n",
    "    y_preds = torch.round(torch.sigmoid(y_logits))\n",
    "    \n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_true=batch_dict['y_target'], y_pred=y_preds)\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "\n",
    "print(f\"Test loss: {train_state['test_loss']:.3f} | Test acc: {train_state['test_acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort weights\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights.to('cpu'), dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "# Top 20 words\n",
    "print(\"Influential words in Positive Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
