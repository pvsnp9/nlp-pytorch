{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import Helper\n",
    "from cbow_dataset import CBOWDataset\n",
    "from cbow_classifier import CBOWClassifier\n",
    "from cbow_vectorizer import CBOWVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel/vectorizer.json\n",
      "\tmodel/model.pth\n",
      "Using CUDA: True\n",
      "Using CUDA: cuda:0\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    cbow_csv=\"../Data/frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model/\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "print(\"Using CUDA: {}\".format(args.device))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "Helper.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "Helper.handle_dirs(args.save_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and loading vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_load_vectorizer(args.cbow_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab), \n",
    "                            embedding_size=args.embedding_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e68762ecfa748c483d449a5f9a2ae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc957e3f4de6422aae03aecf6f0a0975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/1984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481a782de0ed465ca793274317ebfd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting Loop\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = Helper.make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='Training routine', total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_ndex'] = epoch_index\n",
    "        \n",
    "        # iterate over training ds\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = Helper.generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        \n",
    "        # train mode on\n",
    "        classifier.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # training routine in these five steps\n",
    "            \n",
    "            # ---------------------------------------------------\n",
    "            \n",
    "            #step 1 -> zero the grads\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2 -> get logits or y_pred\n",
    "            y_pred = classifier(x=batch_dict['x_data'])\n",
    "            \n",
    "            # step 3 -> compute loss \n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # step 4 -> use optimzzer to take gradient step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # compute the accuracy\n",
    "            acc_t = Helper.compute_accuracy(y_pred,batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # update the bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # iterating over val dataset\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = Helper.generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        \n",
    "        classifier.eval()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(x=batch_dict['x_data']) \n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = Helper.compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = Helper.update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting Loop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = Helper.generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(x=batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = Helper.compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 10.068104887569643;\n",
      "Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...[7.60] - permit\n",
      "...[7.92] - kid\n",
      "...[7.95] - saw\n",
      "...[8.01] - truly\n",
      "...[8.04] - ultimately\n",
      "...[8.07] - confused\n"
     ]
    }
   ],
   "source": [
    "word = input('Enter a word: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "Helper.pretty_print(Helper.get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8318,  1.1497, -1.3499, -1.1848, -1.7517, -2.0525, -0.5165, -0.3856,\n",
       "         1.6346, -2.3498,  1.2516,  1.2691, -0.2066,  0.3899,  0.8535, -0.0461,\n",
       "         1.4486,  0.6616,  1.2856,  1.0166, -0.4041, -2.2251, -0.7742,  0.0843,\n",
       "        -0.5036,  0.4254, -1.4129, -0.2728,  2.2198, -1.7673,  1.3507,  1.7860,\n",
       "         1.2807,  0.0284, -0.5660,  0.6268, -0.3849, -1.0777, -1.3215, -0.5572,\n",
       "        -1.4466,  0.2905, -1.1010, -0.0777, -0.7591,  0.7025,  1.4143, -0.3745,\n",
       "         0.5387,  0.5286], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[word_to_idx['Monster'.lower()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[7.58] - prejudiced\n",
      "...[7.87] - shrivelled\n",
      "...[7.92] - liable\n",
      "...[7.92] - gush\n",
      "...[7.93] - tour\n",
      "...[7.95] - recollect\n",
      "=======monster=======\n",
      "...[7.92] - kid\n",
      "...[7.95] - saw\n",
      "...[8.01] - truly\n",
      "...[8.04] - ultimately\n",
      "...[8.07] - confused\n",
      "...[8.08] - cares\n",
      "=======science=======\n",
      "...[7.13] - impression\n",
      "...[7.22] - mutual\n",
      "...[7.25] - darkened\n",
      "...[7.26] - mist\n",
      "...[7.39] - swelling\n",
      "...[7.45] - tempted\n",
      "=======sickness=======\n",
      "...[6.42] - while\n",
      "...[6.56] - literally\n",
      "...[6.61] - foundations\n",
      "...[6.63] - probabilities\n",
      "...[6.70] - consoles\n",
      "...[6.71] - awoke\n",
      "=======lonely=======\n",
      "...[6.97] - moonlight\n",
      "...[7.00] - unveiled\n",
      "...[7.27] - heartily\n",
      "...[7.28] - ought\n",
      "...[7.32] - undiscovered\n",
      "...[7.35] - bed\n",
      "=======happy=======\n",
      "...[6.56] - bottom\n",
      "...[6.59] - chimney\n",
      "...[6.61] - injury\n",
      "...[6.72] - evening\n",
      "...[6.74] - lingered\n",
      "...[6.79] - chivalry\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    Helper.pretty_print(Helper.get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
